# -*- coding: utf-8 -*-
"""Rekomendasi_Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BQTb35xwuCBpzYw5jcmOlTmU5J2FJAKn

# Import Library

Pertama install pustaka (library) Python bernama Optuna.

Optuna adalah framework sumber terbuka yang digunakan untuk optimasi hyperparameter otomatis (Automated Hyperparameter Optimization)
"""

!pip install optuna

"""Setelah pustaka uptuna berhasil diinstall, selanjutnya import library yang dibutuhkan untuk proyek ini sebagai berikut:"""

import os
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
import optuna
import zipfile
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from tensorflow import keras
from tensorflow.keras import layers
from google.colab import files

"""Selanjutnya extrac file dataset yang sudah di unggah di google colab dengan cara berikut"""

# Nama file ZIP
zip_path = "archive.zip"

# Ekstrak langsung di direktori saat ini
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall()

print("File berhasil diekstrak di direktori saat ini.")

"""Ada pesan menunjukkan file zip berhasil di ekstrak dan selanjutnya ke proses data understanding

# Data Understanding

Dataset ini terdiri dari tiga file CSV, yaitu Books.csv, Ratings.csv, dan Users.csv.

Langkah berikutnya adalah membuka masing-masing file tersebut menggunakan library pandas untuk meninjau isinya.

1. Books.csv
"""

# Load dataset
dfbooks = pd.read_csv('/content/Books.csv')
dfratings = pd.read_csv('/content/Ratings.csv')
dfusers = pd.read_csv('/content/Users.csv')

"""Langkah selanjutnya, melihat isi dari dataset buku, dengan memanggil variabelnya"""

dfbooks

"""Selanjutnya perintah untuk menampilkan ringkasan informasi tentang sebuah DataFrame bernama dfbooks, termasuk jumlah entri, tipe data kolom, dan penggunaan memori."""

dfbooks.info()

"""*
Berdasarkan output di atas, file Books.csv menyimpan informasi mengenai buku dengan total 271.360 entri dan terdiri dari 8 kolom, yaitu:

- ISBN: menyajikan kode unik ISBN untuk setiap buku
- Book-Title: memuat judul buku
- Book-Author: berisi nama penulis buku
- Year-Of-Publication: menunjukkan tahun buku diterbitkan
- Publisher: nama penerbit buku
- Image-URL-S: tautan ke gambar buku berukuran kecil
- Image-URL-M: tautan ke gambar berukuran sedang
- Image-URL-L: tautan ke gambar berukuran besar

2. Ratings.csv

Selanjutnya cara untuk melihat isi dataset rating
"""

dfratings

"""Untuk mengetahui distribusi frekuensi dari setiap nilai rating yang diberikan pengguna, dilakukan pengelompokan data berdasarkan kolom Book-Rating. Kemudian, jumlah baris untuk setiap nilai rating dihitung."""

dfratings.groupby('Book-Rating').count()

"""*Hasilnya menunjukkan berapa kali masing-masing nilai rating (dari 0 hingga 10) diberikan, yang direpresentasikan melalui jumlah entri pada kolom User-ID dan ISBN.

Setelah mengetahui jumlah masing-masing nilai rating yang diberikan oleh pengguna, langkah selanjutnya adalah memvisualisasikannya dalam bentuk grafik batang. Visualisasi ini bertujuan untuk melihat distribusi rating buku secara lebih jelas dan mudah dipahami. Grafik akan menampilkan nilai rating pada sumbu X dan jumlah buku yang menerima rating tersebut pada sumbu Y.
"""

rating_counter = dfratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""*Dari visualisasi, terlihat jelas bahwa sebagian besar buku memiliki rating 0, jumlahnya sangat jauh lebih tinggi dibandingkan rating lainnya. Rating 8, 9, dan 10 memiliki jumlah buku yang cukup signifikan, sementara rating antara 1 hingga 5 memiliki jumlah yang sangat rendah.

Selanjutnya perintah untuk menampilkan ringkasan informasi tentang sebuah DataFrame bernama dfratings, termasuk jumlah entri, tipe data kolom, dan penggunaan memori.
"""

dfratings.info()

"""Perintah selanjutnya menampilkan ringkasan statistik deskriptif dari data rating dalam format angka desimal yang mudah dibaca."""

dfratings.describe().apply(lambda s: s.apply(lambda x: format(x, 'f')))

"""*Berdasarkan output sebelumnya, file Ratings.csv berisi data penilaian buku dari para pengguna. Dataset ini memiliki total 1.149.780 baris dan terdiri dari 3 kolom, yaitu:

- User-ID: kode identifikasi unik untuk setiap pengguna
- ISBN: kode ISBN buku yang telah diberikan nilai oleh pengguna
-Book-Rating: skor rating yang diberikan oleh pengguna dengan rentang nilai antara 0 sampai 10

3. Users.csv

Berikut adalah melihat isi dari datset user
"""

dfusers

"""Selanjutnya perintah untuk menampilkan ringkasan informasi tentang sebuah DataFrame bernama dfusers, termasuk jumlah entri, tipe data kolom, dan penggunaan memori."""

dfusers.info()

"""Langkah selanjutnya menampilkan ringkasan statistik deskriptif dari data pengguna agar kita dapat memahami distribusi dan karakteristik kolom numeriknya."""

dfusers.describe()

"""*Berdasarkan output sebelumnya, file Users.csv berisi 278858 baris dan memiliki 3 kolom, yaitu :

- User-ID : berisi ID unik pengguna
- Location : berisi data lokasi pengguna
- Age : berisi data usia pengguna

# Data Preparation

Sebelum memulai proses pemodelan, penting untuk memastikan data sudah siap dan bersih. Tahap data preparation menjadi fondasi utama agar analisis dan pemodelan berjalan optimal. Berikut ini adalah rangkaian langkah yang kami lakukan dalam menyiapkan data agar siap digunakan.

**Handling Imbalanced Data**

Sebelumnya telah diketahui bahwa data rating tidak seimbang, untuk itu pada tahap ini saya mencoba untuk menghapus data rating 0.
"""

dfratings.drop(dfratings[dfratings["Book-Rating"] == 0].index, inplace=True)

"""Berikut ini adalah jumlah data setelah di-drop"""

dfratings.shape

"""Cek kembali dfratings setelah di drop"""

dfratings

"""Kode berikut akan membuat grafik batang untuk memperlihatkan berapa banyak buku yang mendapatkan setiap nilai rating dari pengguna.


"""

rating_counter = dfratings.groupby('Book-Rating').count()
plt.figure(figsize=(10,5))
plt.title('Jumlah Rating Buku yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah Buku')
plt.bar(rating_counter.index, rating_counter['ISBN'])
plt.grid(True)
plt.show()

"""*Gambar kedua ini menampilkan histogram distribusi rating buku setelah menghapus rating bernilai 0. Berbeda dari visualisasi sebelumnya yang didominasi oleh rating 0 (kemungkinan data kosong), grafik ini kini menyoroti bagaimana pengguna sesungguhnya memberi rating. Terlihat jelas bahwa rating 8 merupakan yang paling banyak diberikan, diikuti oleh rating 7 dan 10. Sebaliknya, rating di bawah 5, khususnya 1 hingga 4, sangat jarang muncul, menunjukkan bahwa sebagian besar rating positif atau menengah yang diberikan pengguna.

**Encoding Data**

Encoding dilakukan untuk mengubah User-ID dan ISBN menjadi angka indeks dalam bentuk bilangan bulat.
"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = dfratings['User-ID'].unique().tolist()

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn_list = dfratings['ISBN'].unique().tolist()

# Melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_list)}

# Melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_list)}

"""Selanjutnya itu hasil dari encoding akan dimapping ke dataframe dfratings"""

# Mapping userID ke dataframe user
dfratings['user'] = dfratings['User-ID'].map(user_to_user_encoded)

# Mapping userID ke dataframe user
dfratings['book'] = dfratings['ISBN'].map(isbn_to_isbn_encoded)

"""Cek dfratings setelah encoding"""

dfratings

"""Cek juga ratings.info() setelah endocing"""

dfratings.info()

"""*Setelah encoding, data memiliki 433.671 baris dan 5 kolom, dengan kolom user dan book sebagai representasi angka dari User-ID dan ISBN untuk memudahkan pemrosesan data.

**Randomize Dataset**

Langkah berikutnya adalah melakukan pengacakan data agar distribusinya menjadi acak dan lebih merata, sehingga analisis dan pemodelan dapat berjalan dengan lebih baik.
"""

df = dfratings.sample(frac=1, random_state=42)
df

"""**Data Standardization and Splitting**

Setelah data diacak secara acak, dataset kemudian dibagi menjadi dua bagian, yakni 80% untuk pelatihan model dan 20% untuk validasi.

Selain itu, nilai rating yang awalnya berada dalam skala 0 sampai 10, diubah menjadi rentang 0 sampai 1 melalui proses standarisasi agar proses pelatihan model menjadi lebih efektif dan stabil.
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah resto
num_isbn = len(isbn_encoded_to_isbn)
print(num_isbn)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum Book-Rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal Book-Rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of ISBN: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_isbn, min_rating, max_rating
))

"""*Output ini menunjukkan bahwa dataset berisi 77.805 pengguna unik, 185.973 buku berbeda, dengan nilai rating mulai dari 1 hingga 10.

Selanjutnya untuk menyiapkan data fitur dan target dengan melakukan normalisasi rating serta membagi dataset menjadi data pelatihan dan validasi agar siap digunakan dalam proses pemodelan sebagai berikut:
"""

# Membuat variabel x untuk mencocokkan data user dan book menjadi satu value
x = df[['user', 'book']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""# Modelling

**Membuat Kelas RecommenderNet**

Selanjutnya mendefinisikan model rekomendasi berbasis embedding yang menggabungkan representasi pengguna dan buku untuk memprediksi rating dalam rentang 0 hingga 1 menggunakan fungsi aktivasi sigmoid.
"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn = num_isbn
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_isbn,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_isbn, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""**Hyperparameter Tuning**

Untuk memperoleh performa model yang maksimal, proyek ini memanfaatkan library Optuna sebagai alat otomatis dalam mencari nilai hyperparameter terbaik, khususnya untuk menentukan ukuran embedding (embedding_size) yang optimal.
"""

def objective(trial):
    tf.keras.backend.clear_session()
    model = RecommenderNet(num_users=num_users, num_isbn=num_isbn, embedding_size=trial.suggest_int('embedding_size', 1, 15))

    # model compile
    model.compile(
        loss = tf.keras.losses.BinaryCrossentropy(),
        optimizer = keras.optimizers.Adam(learning_rate=0.001),
        metrics=[tf.keras.metrics.RootMeanSquaredError()]
    )

    model.fit(
        x = x_train,
        y = y_train,
        batch_size=200,
        epochs = 1,
        validation_data = (x_val, y_val)
    )

    y_pred= model.predict(x_val)

    # Calculate Mean Squared Error
    mse = mean_squared_error(y_val, y_pred)
    # Calculate Root Mean Squared Error
    rmse = np.sqrt(mse)

    return rmse

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=15, timeout=500)

print("Number of finished trials: {}".format(len(study.trials)))

print("Best trial:")
trial = study.best_trial

print("  Value: {}".format(trial.value))

print("  Params: ")
for key, value in trial.params.items():
    print("    {}: {}".format(key, value))

"""Berikut untuk membersihkan sesi sebelumnya dan menginisialisasi model rekomendasi dengan nilai hyperparameter terbaik sebelum dilakukan proses pelatihan."""

tf.keras.backend.clear_session()

# Menerapkan nilai parameter paling optimal dari optuna
BEST_EMBEDDING_SIZE = 1

model = RecommenderNet(num_users, num_isbn, BEST_EMBEDDING_SIZE)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""**Melatih Model**

Selanjutnya, dilakukan proses pelatihan model menggunakan data yang telah dibagi sebelumnya untuk mempelajari pola interaksi antara pengguna dan buku.
"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size=64,
    epochs = 10,
    validation_data = (x_val, y_val)
)

"""# Evaluasi

Langkah selanjutnya adalah mengevaluasi performa model dengan memvisualisasikan nilai Root Mean Squared Error (RMSE) pada data pelatihan dan validasi selama proses training.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.grid(True)
plt.show()

"""*Berdasarkan hasil evaluasi, model yang dikembangkan berhasil mencapai nilai Root Mean Squared Error (RMSE) sebesar 0.186, yang menunjukkan bahwa prediksi rating yang dihasilkan cukup akurat dan mendekati nilai sebenarnya.

**Mendapatkan Rekomendasi**

Langkah selanjutnya adalah menghasilkan rekomendasi buku untuk pengguna secara acak berdasarkan model yang telah dilatih, dengan memprediksi rating tertinggi dari buku-buku yang belum pernah dibaca oleh pengguna tersebut.
"""

books_df = dfbooks
df = pd.read_csv('/content/Ratings.csv')

# Mengambil sample user
user_id = df['User-ID'].sample(1).iloc[0]
book_read_by_user = df[df['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = books_df[~books_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

"""Berikut adalah proses untuk menampilkan daftar buku dengan rating tertinggi yang telah dibaca oleh pengguna, sekaligus menghasilkan 10 rekomendasi buku terbaik yang belum pernah dibaca berdasarkan prediksi model."""

# Prediksi rating buku yang belum dibaca oleh user
dfratings = model.predict(user_book_array).flatten()

# Mengambil 10 buku dengan prediksi rating tertinggi
top_ratings_indices = dfratings.argsort()[-10:][::-1]
recommended_book_isbns = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

print(f"\n📚 Showing recommendations for user ID: {user_id}")
print("=" * 40)

# Menampilkan buku dengan rating tertinggi dari user
print("\n⭐ Buku dengan rating tertinggi dari user:")
print("-" * 40)
top_book_user = (
    book_read_by_user.sort_values(by='Book-Rating', ascending=False)
    .head(5)
    .ISBN.values
)

book_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    # Access 'Book-Title' (index 1) and 'Book-Author' (index 2) by position
    print(f"📖 {row[1]} — {row[2]}")

# Menampilkan 10 rekomendasi buku teratas
print("\n🎯 Top 10 rekomendasi buku untuk user:")
print("-" * 40)
recommended_books = books_df[books_df['ISBN'].isin(recommended_book_isbns)]
for i, row in enumerate(recommended_books.itertuples(), start=1):
    # Access 'Book-Title' (index 1) and 'Book-Author' (index 2) by position
    print(f"{i}. {row[1]} — {row[2]}")

"""Berdasarkan hasil output tersebut, dapat disimpulkan bahwa sistem rekomendasi berhasil memberikan saran buku yang relevan dan beragam untuk pengguna dengan ID 235105. Sistem menampilkan 5 buku yang sebelumnya telah diberi rating tinggi oleh pengguna, menunjukkan preferensinya terhadap novel, biografi, dan literatur pendidikan.

Sementara itu, 10 rekomendasi buku yang diberikan sistem mencakup berbagai genre populer seperti fantasi (The Lord of the Rings, Harry Potter), fiksi kontemporer (My Sister's Keeper), hingga koleksi komik dan buku inspiratif (Calvin and Hobbes, The Giving Tree). Hal ini menunjukkan bahwa model berhasil mempelajari pola kesukaan pengguna dan menyarankan buku-buku dengan potensi rating tinggi yang belum dibaca.
"""